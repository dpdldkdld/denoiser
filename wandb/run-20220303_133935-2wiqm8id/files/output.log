Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            [(None, 129, 8, 1)]  0
__________________________________________________________________________________________________
zero_padding2d (ZeroPadding2D)  (None, 137, 8, 1)    0           input_1[0][0]
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 129, 1, 18)   1296        zero_padding2d[0][0]
__________________________________________________________________________________________________
activation (Activation)         (None, 129, 1, 18)   0           conv2d[0][0]
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 129, 1, 18)   72          activation[0][0]
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 129, 1, 30)   2700        batch_normalization[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 129, 1, 30)   0           conv2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 129, 1, 30)   120         activation_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 129, 1, 8)    2160        batch_normalization_1[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 129, 1, 8)    0           conv2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 129, 1, 8)    32          activation_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 129, 1, 18)   1296        batch_normalization_2[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 129, 1, 18)   0           conv2d_3[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 129, 1, 18)   72          activation_3[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 129, 1, 30)   2700        batch_normalization_3[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 129, 1, 30)   0           conv2d_4[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 129, 1, 30)   120         activation_4[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 129, 1, 8)    2160        batch_normalization_4[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 129, 1, 8)    0           conv2d_5[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 129, 1, 8)    32          activation_5[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 129, 1, 18)   1296        batch_normalization_5[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 129, 1, 18)   0           conv2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 129, 1, 18)   72          activation_6[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 129, 1, 30)   2700        batch_normalization_6[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 129, 1, 30)   0           conv2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 129, 1, 30)   120         activation_7[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 129, 1, 8)    2160        batch_normalization_7[0][0]
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 129, 1, 8)    0           conv2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 129, 1, 8)    32          activation_8[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 129, 1, 18)   1296        batch_normalization_8[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 129, 1, 18)   0           conv2d_9[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 129, 1, 18)   72          activation_9[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 129, 1, 30)   2700        batch_normalization_9[0][0]
__________________________________________________________________________________________________
tf.__operators__.add (TFOpLambd (None, 129, 1, 30)   0           conv2d_10[0][0]
                                                                 conv2d_4[0][0]
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 129, 1, 30)   0           tf.__operators__.add[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 129, 1, 30)   120         activation_10[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 129, 1, 8)    2160        batch_normalization_10[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 129, 1, 8)    0           conv2d_11[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 129, 1, 8)    32          activation_11[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 129, 1, 18)   1296        batch_normalization_11[0][0]
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 129, 1, 18)   0           conv2d_12[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 129, 1, 18)   72          activation_12[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 129, 1, 30)   2700        batch_normalization_12[0][0]
__________________________________________________________________________________________________
tf.__operators__.add_1 (TFOpLam (None, 129, 1, 30)   0           conv2d_13[0][0]
                                                                 conv2d_1[0][0]
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 129, 1, 30)   0           tf.__operators__.add_1[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 129, 1, 30)   120         activation_13[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 129, 1, 8)    2160        batch_normalization_13[0][0]
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 129, 1, 8)    0           conv2d_14[0][0]
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 129, 1, 8)    32          activation_14[0][0]
__________________________________________________________________________________________________
spatial_dropout2d (SpatialDropo (None, 129, 1, 8)    0           batch_normalization_14[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 129, 1, 1)    1033        spatial_dropout2d[0][0]
==================================================================================================
Total params: 32,933
Trainable params: 32,373
Non-trainable params: 560
__________________________________________________________________________________________________
('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')
2022-03-03 13:40:12.626528: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2022-03-03 13:40:12.627100: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2299995000 Hz
2022-03-03 13:40:13.026798: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7
2022-03-03 13:40:14.192577: W tensorflow/stream_executor/gpu/asm_compiler.cc:98] *** WARNING *** You are using ptxas 9.1.108, which is older than 9.2.88. ptxas 9.x before 9.2.88 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.
You do not need to update to CUDA 9.2.88; cherry-picking the ptxas binary is sufficient.
2022-03-03 13:40:14.291901: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10


































2210/2210 [==============================] - 72s 32ms/step - loss: 0.6352 - rmse: 0.7970
Baseline accuracy 0.6346789598464966
2022-03-03 13:41:36.546962: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.
2022-03-03 13:41:36.547012: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.
2022-03-03 13:41:36.547054: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1365] Profiler found 1 GPUs
2022-03-03 13:41:36.551789: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcupti.so.10.1
2022-03-03 13:41:36.652328: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1415] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI_ERROR_INSUFFICIENT_PRIVILEGES
2022-03-03 13:41:36.652743: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.
Epoch 1/400









600/600 [==============================] - 83s 136ms/step - loss: 0.3853 - rmse: 0.6207 - val_loss: 0.2489 - val_rmse: 0.4989
Epoch 2/400








600/600 [==============================] - 85s 141ms/step - loss: 0.2306 - rmse: 0.4802 - val_loss: 0.2088 - val_rmse: 0.4569
Epoch 3/400









600/600 [==============================] - 84s 139ms/step - loss: 0.2101 - rmse: 0.4584 - val_loss: 0.2027 - val_rmse: 0.4502
Epoch 4/400








600/600 [==============================] - 81s 135ms/step - loss: 0.2005 - rmse: 0.4478 - val_loss: 0.1987 - val_rmse: 0.4458
Epoch 5/400








600/600 [==============================] - 80s 133ms/step - loss: 0.1906 - rmse: 0.4365 - val_loss: 0.1976 - val_rmse: 0.4445
Epoch 6/400









600/600 [==============================] - 80s 134ms/step - loss: 0.1870 - rmse: 0.4325 - val_loss: 0.1956 - val_rmse: 0.4423
Epoch 7/400







600/600 [==============================] - 80s 133ms/step - loss: 0.1815 - rmse: 0.4260 - val_loss: 0.1811 - val_rmse: 0.4256
Epoch 8/400






600/600 [==============================] - 79s 132ms/step - loss: 0.1774 - rmse: 0.4212 - val_loss: 0.1810 - val_rmse: 0.4255
Epoch 9/400







600/600 [==============================] - 78s 130ms/step - loss: 0.1731 - rmse: 0.4161 - val_loss: 0.1780 - val_rmse: 0.4219
Epoch 10/400



600/600 [==============================] - 73s 122ms/step - loss: 0.1707 - rmse: 0.4131 - val_loss: 0.1803 - val_rmse: 0.4246
Epoch 11/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1727 - rmse: 0.4156 - val_loss: 0.1973 - val_rmse: 0.4442
Epoch 12/400



600/600 [==============================] - 74s 124ms/step - loss: 0.1688 - rmse: 0.4108 - val_loss: 0.1744 - val_rmse: 0.4176
Epoch 13/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1673 - rmse: 0.4090 - val_loss: 0.1887 - val_rmse: 0.4344
Epoch 14/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1663 - rmse: 0.4078 - val_loss: 0.1793 - val_rmse: 0.4235
Epoch 15/400



600/600 [==============================] - 74s 124ms/step - loss: 0.1647 - rmse: 0.4058 - val_loss: 0.1957 - val_rmse: 0.4424
Epoch 16/400



600/600 [==============================] - 77s 128ms/step - loss: 0.1609 - rmse: 0.4011 - val_loss: 0.1751 - val_rmse: 0.4184
Epoch 17/400




600/600 [==============================] - 74s 123ms/step - loss: 0.1616 - rmse: 0.4020 - val_loss: 0.1786 - val_rmse: 0.4226
Epoch 18/400



600/600 [==============================] - 76s 128ms/step - loss: 0.1621 - rmse: 0.4026 - val_loss: 0.1749 - val_rmse: 0.4182
Epoch 19/400



600/600 [==============================] - 76s 127ms/step - loss: 0.1635 - rmse: 0.4043 - val_loss: 0.1816 - val_rmse: 0.4261
Epoch 20/400



600/600 [==============================] - 74s 124ms/step - loss: 0.1582 - rmse: 0.3977 - val_loss: 0.2016 - val_rmse: 0.4490
Epoch 21/400




600/600 [==============================] - 74s 124ms/step - loss: 0.1608 - rmse: 0.4011 - val_loss: 0.1710 - val_rmse: 0.4135
Epoch 22/400




600/600 [==============================] - 75s 125ms/step - loss: 0.1579 - rmse: 0.3973 - val_loss: 0.1818 - val_rmse: 0.4264
Epoch 23/400



600/600 [==============================] - 73s 123ms/step - loss: 0.1574 - rmse: 0.3968 - val_loss: 0.1766 - val_rmse: 0.4202
Epoch 24/400



600/600 [==============================] - 73s 122ms/step - loss: 0.1555 - rmse: 0.3943 - val_loss: 0.2020 - val_rmse: 0.4494
Epoch 25/400



600/600 [==============================] - 73s 123ms/step - loss: 0.1566 - rmse: 0.3957 - val_loss: 0.1702 - val_rmse: 0.4125
Epoch 26/400



600/600 [==============================] - 74s 124ms/step - loss: 0.1581 - rmse: 0.3976 - val_loss: 0.1796 - val_rmse: 0.4238
Epoch 27/400



600/600 [==============================] - 73s 122ms/step - loss: 0.1559 - rmse: 0.3949 - val_loss: 0.1643 - val_rmse: 0.4054
Epoch 28/400




600/600 [==============================] - 75s 125ms/step - loss: 0.1561 - rmse: 0.3951 - val_loss: 0.1685 - val_rmse: 0.4105
Epoch 29/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1531 - rmse: 0.3913 - val_loss: 0.1704 - val_rmse: 0.4128
Epoch 30/400




600/600 [==============================] - 74s 124ms/step - loss: 0.1557 - rmse: 0.3946 - val_loss: 0.1671 - val_rmse: 0.4087
Epoch 31/400



600/600 [==============================] - 74s 124ms/step - loss: 0.1516 - rmse: 0.3893 - val_loss: 0.1694 - val_rmse: 0.4116
Epoch 32/400



600/600 [==============================] - 76s 127ms/step - loss: 0.1526 - rmse: 0.3906 - val_loss: 0.1721 - val_rmse: 0.4148
Epoch 33/400



600/600 [==============================] - 74s 124ms/step - loss: 0.1555 - rmse: 0.3944 - val_loss: 0.1670 - val_rmse: 0.4086
Epoch 34/400



600/600 [==============================] - 76s 127ms/step - loss: 0.1525 - rmse: 0.3905 - val_loss: 0.1719 - val_rmse: 0.4146
Epoch 35/400



600/600 [==============================] - 76s 127ms/step - loss: 0.1536 - rmse: 0.3919 - val_loss: 0.1905 - val_rmse: 0.4364
Epoch 36/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1521 - rmse: 0.3901 - val_loss: 0.1664 - val_rmse: 0.4079
Epoch 37/400



600/600 [==============================] - 74s 124ms/step - loss: 0.1515 - rmse: 0.3892 - val_loss: 0.1764 - val_rmse: 0.4200
Epoch 38/400



600/600 [==============================] - 74s 124ms/step - loss: 0.1505 - rmse: 0.3880 - val_loss: 0.1626 - val_rmse: 0.4032
Epoch 39/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1509 - rmse: 0.3885 - val_loss: 0.1701 - val_rmse: 0.4124
Epoch 40/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1520 - rmse: 0.3899 - val_loss: 0.1714 - val_rmse: 0.4140
Epoch 41/400




600/600 [==============================] - 74s 123ms/step - loss: 0.1519 - rmse: 0.3897 - val_loss: 0.1716 - val_rmse: 0.4143
Epoch 42/400




600/600 [==============================] - 74s 123ms/step - loss: 0.1507 - rmse: 0.3883 - val_loss: 0.1782 - val_rmse: 0.4222
Epoch 43/400



600/600 [==============================] - 75s 124ms/step - loss: 0.1517 - rmse: 0.3895 - val_loss: 0.1627 - val_rmse: 0.4034
Epoch 44/400




600/600 [==============================] - 74s 124ms/step - loss: 0.1501 - rmse: 0.3875 - val_loss: 0.1657 - val_rmse: 0.4071
Epoch 45/400




600/600 [==============================] - 74s 124ms/step - loss: 0.1486 - rmse: 0.3855 - val_loss: 0.1685 - val_rmse: 0.4105
Epoch 46/400




600/600 [==============================] - 74s 123ms/step - loss: 0.1488 - rmse: 0.3858 - val_loss: 0.1653 - val_rmse: 0.4066
Epoch 47/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1498 - rmse: 0.3870 - val_loss: 0.1662 - val_rmse: 0.4076
Epoch 48/400



600/600 [==============================] - 76s 127ms/step - loss: 0.1512 - rmse: 0.3889 - val_loss: 0.1655 - val_rmse: 0.4069
Epoch 49/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1494 - rmse: 0.3865 - val_loss: 0.1712 - val_rmse: 0.4138
Epoch 50/400



600/600 [==============================] - 76s 127ms/step - loss: 0.1500 - rmse: 0.3873 - val_loss: 0.1649 - val_rmse: 0.4061
Epoch 51/400



600/600 [==============================] - 78s 131ms/step - loss: 0.1474 - rmse: 0.3839 - val_loss: 0.1647 - val_rmse: 0.4059
Epoch 52/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1498 - rmse: 0.3870 - val_loss: 0.1679 - val_rmse: 0.4098
Epoch 53/400




600/600 [==============================] - 73s 123ms/step - loss: 0.1466 - rmse: 0.3829 - val_loss: 0.1637 - val_rmse: 0.4046
Epoch 54/400



600/600 [==============================] - 73s 122ms/step - loss: 0.1475 - rmse: 0.3841 - val_loss: 0.1897 - val_rmse: 0.4355
Epoch 55/400



600/600 [==============================] - 74s 124ms/step - loss: 0.1507 - rmse: 0.3882 - val_loss: 0.1651 - val_rmse: 0.4063
Epoch 56/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1480 - rmse: 0.3848 - val_loss: 0.1637 - val_rmse: 0.4046
Epoch 57/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1483 - rmse: 0.3850 - val_loss: 0.1618 - val_rmse: 0.4022
Epoch 58/400



600/600 [==============================] - 74s 124ms/step - loss: 0.1479 - rmse: 0.3846 - val_loss: 0.1713 - val_rmse: 0.4138
Epoch 59/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1475 - rmse: 0.3841 - val_loss: 0.1734 - val_rmse: 0.4164
Epoch 60/400



600/600 [==============================] - 74s 124ms/step - loss: 0.1458 - rmse: 0.3818 - val_loss: 0.1629 - val_rmse: 0.4036
Epoch 61/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1457 - rmse: 0.3817 - val_loss: 0.1619 - val_rmse: 0.4024
Epoch 62/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1497 - rmse: 0.3869 - val_loss: 0.1734 - val_rmse: 0.4164
Epoch 63/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1471 - rmse: 0.3835 - val_loss: 0.1615 - val_rmse: 0.4019
Epoch 64/400



600/600 [==============================] - 77s 129ms/step - loss: 0.1478 - rmse: 0.3845 - val_loss: 0.1647 - val_rmse: 0.4059
Epoch 65/400



600/600 [==============================] - 74s 124ms/step - loss: 0.1463 - rmse: 0.3825 - val_loss: 0.1650 - val_rmse: 0.4063
Epoch 66/400




600/600 [==============================] - 76s 127ms/step - loss: 0.1475 - rmse: 0.3841 - val_loss: 0.1716 - val_rmse: 0.4143
Epoch 67/400




600/600 [==============================] - 76s 127ms/step - loss: 0.1446 - rmse: 0.3802 - val_loss: 0.1618 - val_rmse: 0.4023
Epoch 68/400




600/600 [==============================] - 74s 124ms/step - loss: 0.1461 - rmse: 0.3822 - val_loss: 0.1659 - val_rmse: 0.4073
Epoch 69/400




600/600 [==============================] - 74s 124ms/step - loss: 0.1464 - rmse: 0.3827 - val_loss: 0.1636 - val_rmse: 0.4044
Epoch 70/400




600/600 [==============================] - 74s 124ms/step - loss: 0.1486 - rmse: 0.3855 - val_loss: 0.1669 - val_rmse: 0.4085
Epoch 71/400




600/600 [==============================] - 74s 124ms/step - loss: 0.1449 - rmse: 0.3807 - val_loss: 0.1748 - val_rmse: 0.4181
Epoch 72/400




600/600 [==============================] - 75s 125ms/step - loss: 0.1472 - rmse: 0.3836 - val_loss: 0.1594 - val_rmse: 0.3993
Epoch 73/400



600/600 [==============================] - 73s 123ms/step - loss: 0.1457 - rmse: 0.3817 - val_loss: 0.1615 - val_rmse: 0.4019
Epoch 74/400



600/600 [==============================] - 73s 122ms/step - loss: 0.1453 - rmse: 0.3812 - val_loss: 0.1657 - val_rmse: 0.4070
Epoch 75/400




600/600 [==============================] - 73s 123ms/step - loss: 0.1444 - rmse: 0.3800 - val_loss: 0.1768 - val_rmse: 0.4204
Epoch 76/400



600/600 [==============================] - 74s 124ms/step - loss: 0.1451 - rmse: 0.3809 - val_loss: 0.1660 - val_rmse: 0.4074
Epoch 77/400




600/600 [==============================] - 74s 123ms/step - loss: 0.1472 - rmse: 0.3837 - val_loss: 0.1671 - val_rmse: 0.4088
Epoch 78/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1458 - rmse: 0.3818 - val_loss: 0.1624 - val_rmse: 0.4030
Epoch 79/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1456 - rmse: 0.3816 - val_loss: 0.1736 - val_rmse: 0.4167
Epoch 80/400



600/600 [==============================] - 77s 129ms/step - loss: 0.1438 - rmse: 0.3793 - val_loss: 0.1597 - val_rmse: 0.3997
Epoch 81/400




600/600 [==============================] - 73s 122ms/step - loss: 0.1462 - rmse: 0.3824 - val_loss: 0.1612 - val_rmse: 0.4015
Epoch 82/400



600/600 [==============================] - 76s 127ms/step - loss: 0.1432 - rmse: 0.3784 - val_loss: 0.1569 - val_rmse: 0.3961
Epoch 83/400



600/600 [==============================] - 76s 128ms/step - loss: 0.1437 - rmse: 0.3790 - val_loss: 0.1638 - val_rmse: 0.4047
Epoch 84/400



600/600 [==============================] - 73s 123ms/step - loss: 0.1469 - rmse: 0.3833 - val_loss: 0.1606 - val_rmse: 0.4007
Epoch 85/400



600/600 [==============================] - 74s 124ms/step - loss: 0.1439 - rmse: 0.3793 - val_loss: 0.1882 - val_rmse: 0.4338
Epoch 86/400



600/600 [==============================] - 73s 122ms/step - loss: 0.1452 - rmse: 0.3811 - val_loss: 0.1623 - val_rmse: 0.4028
Epoch 87/400




600/600 [==============================] - 73s 123ms/step - loss: 0.1443 - rmse: 0.3799 - val_loss: 0.1622 - val_rmse: 0.4028
Epoch 88/400




600/600 [==============================] - 73s 122ms/step - loss: 0.1438 - rmse: 0.3792 - val_loss: 0.1718 - val_rmse: 0.4145
Epoch 89/400



600/600 [==============================] - 74s 124ms/step - loss: 0.1434 - rmse: 0.3786 - val_loss: 0.1613 - val_rmse: 0.4016
Epoch 90/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1433 - rmse: 0.3785 - val_loss: 0.1705 - val_rmse: 0.4130
Epoch 91/400




600/600 [==============================] - 74s 123ms/step - loss: 0.1452 - rmse: 0.3811 - val_loss: 0.1624 - val_rmse: 0.4030
Epoch 92/400



600/600 [==============================] - 74s 124ms/step - loss: 0.1447 - rmse: 0.3804 - val_loss: 0.1686 - val_rmse: 0.4106
Epoch 93/400



600/600 [==============================] - 75s 125ms/step - loss: 0.1441 - rmse: 0.3796 - val_loss: 0.1735 - val_rmse: 0.4165
Epoch 94/400



600/600 [==============================] - 74s 124ms/step - loss: 0.1448 - rmse: 0.3806 - val_loss: 0.1652 - val_rmse: 0.4064
Epoch 95/400



600/600 [==============================] - 74s 124ms/step - loss: 0.1435 - rmse: 0.3788 - val_loss: 0.1587 - val_rmse: 0.3984
Epoch 96/400



600/600 [==============================] - 76s 127ms/step - loss: 0.1424 - rmse: 0.3774 - val_loss: 0.1577 - val_rmse: 0.3971
Epoch 97/400



600/600 [==============================] - 75s 124ms/step - loss: 0.1426 - rmse: 0.3777 - val_loss: 0.1679 - val_rmse: 0.4098
Epoch 98/400



600/600 [==============================] - 76s 127ms/step - loss: 0.1437 - rmse: 0.3791 - val_loss: 0.1645 - val_rmse: 0.4056
Epoch 99/400



600/600 [==============================] - 76s 127ms/step - loss: 0.1455 - rmse: 0.3814 - val_loss: 0.1616 - val_rmse: 0.4020
Epoch 100/400



600/600 [==============================] - 74s 124ms/step - loss: 0.1434 - rmse: 0.3786 - val_loss: 0.1665 - val_rmse: 0.4081
Epoch 101/400




600/600 [==============================] - 75s 124ms/step - loss: 0.1441 - rmse: 0.3796 - val_loss: 0.1663 - val_rmse: 0.4078
Epoch 102/400



600/600 [==============================] - 73s 122ms/step - loss: 0.1416 - rmse: 0.3763 - val_loss: 0.1621 - val_rmse: 0.4026
Epoch 103/400



600/600 [==============================] - 73s 123ms/step - loss: 0.1440 - rmse: 0.3795 - val_loss: 0.1608 - val_rmse: 0.4010
Epoch 104/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1418 - rmse: 0.3765 - val_loss: 0.1763 - val_rmse: 0.4198
Epoch 105/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1418 - rmse: 0.3766 - val_loss: 0.1654 - val_rmse: 0.4067
Epoch 106/400



600/600 [==============================] - 73s 123ms/step - loss: 0.1449 - rmse: 0.3806 - val_loss: 0.1626 - val_rmse: 0.4033
Epoch 107/400




600/600 [==============================] - 73s 122ms/step - loss: 0.1428 - rmse: 0.3779 - val_loss: 0.1752 - val_rmse: 0.4186
Epoch 108/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1430 - rmse: 0.3782 - val_loss: 0.1618 - val_rmse: 0.4023
Epoch 109/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1428 - rmse: 0.3779 - val_loss: 0.1650 - val_rmse: 0.4063
Epoch 110/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1430 - rmse: 0.3781 - val_loss: 0.1564 - val_rmse: 0.3955
Epoch 111/400



600/600 [==============================] - 73s 123ms/step - loss: 0.1412 - rmse: 0.3758 - val_loss: 0.1635 - val_rmse: 0.4043
Epoch 112/400




600/600 [==============================] - 76s 127ms/step - loss: 0.1406 - rmse: 0.3749 - val_loss: 0.1616 - val_rmse: 0.4020
Epoch 113/400



600/600 [==============================] - 73s 123ms/step - loss: 0.1452 - rmse: 0.3810 - val_loss: 0.1630 - val_rmse: 0.4037
Epoch 114/400



600/600 [==============================] - 77s 128ms/step - loss: 0.1422 - rmse: 0.3771 - val_loss: 0.1734 - val_rmse: 0.4164
Epoch 115/400




600/600 [==============================] - 76s 126ms/step - loss: 0.1429 - rmse: 0.3780 - val_loss: 0.1575 - val_rmse: 0.3969
Epoch 116/400




600/600 [==============================] - 74s 123ms/step - loss: 0.1416 - rmse: 0.3762 - val_loss: 0.1634 - val_rmse: 0.4043
Epoch 117/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1429 - rmse: 0.3780 - val_loss: 0.1715 - val_rmse: 0.4141
Epoch 118/400



600/600 [==============================] - 74s 124ms/step - loss: 0.1402 - rmse: 0.3745 - val_loss: 0.1575 - val_rmse: 0.3968
Epoch 119/400



600/600 [==============================] - 73s 122ms/step - loss: 0.1418 - rmse: 0.3766 - val_loss: 0.1620 - val_rmse: 0.4025
Epoch 120/400




600/600 [==============================] - 74s 123ms/step - loss: 0.1421 - rmse: 0.3770 - val_loss: 0.1639 - val_rmse: 0.4048
Epoch 121/400




600/600 [==============================] - 74s 123ms/step - loss: 0.1443 - rmse: 0.3799 - val_loss: 0.1620 - val_rmse: 0.4024
Epoch 122/400



600/600 [==============================] - 74s 124ms/step - loss: 0.1406 - rmse: 0.3750 - val_loss: 0.1607 - val_rmse: 0.4008
Epoch 123/400




600/600 [==============================] - 73s 122ms/step - loss: 0.1430 - rmse: 0.3781 - val_loss: 0.1588 - val_rmse: 0.3985
Epoch 124/400



600/600 [==============================] - 73s 122ms/step - loss: 0.1415 - rmse: 0.3761 - val_loss: 0.1594 - val_rmse: 0.3992
Epoch 125/400




600/600 [==============================] - 74s 124ms/step - loss: 0.1411 - rmse: 0.3756 - val_loss: 0.1567 - val_rmse: 0.3959
Epoch 126/400




600/600 [==============================] - 74s 124ms/step - loss: 0.1402 - rmse: 0.3744 - val_loss: 0.1748 - val_rmse: 0.4180
Epoch 127/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1417 - rmse: 0.3764 - val_loss: 0.1627 - val_rmse: 0.4034
Epoch 128/400




600/600 [==============================] - 76s 127ms/step - loss: 0.1432 - rmse: 0.3784 - val_loss: 0.1646 - val_rmse: 0.4057
Epoch 129/400




600/600 [==============================] - 74s 123ms/step - loss: 0.1420 - rmse: 0.3768 - val_loss: 0.1612 - val_rmse: 0.4015
Epoch 130/400




600/600 [==============================] - 76s 127ms/step - loss: 0.1420 - rmse: 0.3768 - val_loss: 0.1676 - val_rmse: 0.4093
Epoch 131/400



600/600 [==============================] - 76s 127ms/step - loss: 0.1404 - rmse: 0.3747 - val_loss: 0.1585 - val_rmse: 0.3982
Epoch 132/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1424 - rmse: 0.3773 - val_loss: 0.1576 - val_rmse: 0.3970
Epoch 133/400




600/600 [==============================] - 74s 123ms/step - loss: 0.1397 - rmse: 0.3737 - val_loss: 0.1625 - val_rmse: 0.4031
Epoch 134/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1403 - rmse: 0.3746 - val_loss: 0.1585 - val_rmse: 0.3981
Epoch 135/400



600/600 [==============================] - 75s 125ms/step - loss: 0.1437 - rmse: 0.3791 - val_loss: 0.1626 - val_rmse: 0.4032
Epoch 136/400



600/600 [==============================] - 73s 123ms/step - loss: 0.1405 - rmse: 0.3749 - val_loss: 0.1692 - val_rmse: 0.4114
Epoch 137/400



600/600 [==============================] - 73s 122ms/step - loss: 0.1420 - rmse: 0.3769 - val_loss: 0.1588 - val_rmse: 0.3985
Epoch 138/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1410 - rmse: 0.3755 - val_loss: 0.1712 - val_rmse: 0.4138
Epoch 139/400




600/600 [==============================] - 74s 124ms/step - loss: 0.1413 - rmse: 0.3759 - val_loss: 0.1659 - val_rmse: 0.4073
Epoch 140/400




600/600 [==============================] - 74s 124ms/step - loss: 0.1400 - rmse: 0.3742 - val_loss: 0.1599 - val_rmse: 0.3999
Epoch 141/400




600/600 [==============================] - 73s 123ms/step - loss: 0.1401 - rmse: 0.3743 - val_loss: 0.1641 - val_rmse: 0.4051
Epoch 142/400



600/600 [==============================] - 74s 124ms/step - loss: 0.1421 - rmse: 0.3770 - val_loss: 0.1630 - val_rmse: 0.4037
Epoch 143/400



600/600 [==============================] - 75s 124ms/step - loss: 0.1417 - rmse: 0.3765 - val_loss: 0.1598 - val_rmse: 0.3997
Epoch 144/400




600/600 [==============================] - 74s 124ms/step - loss: 0.1408 - rmse: 0.3753 - val_loss: 0.1625 - val_rmse: 0.4031
Epoch 145/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1414 - rmse: 0.3760 - val_loss: 0.1568 - val_rmse: 0.3960
Epoch 146/400




600/600 [==============================] - 76s 127ms/step - loss: 0.1405 - rmse: 0.3748 - val_loss: 0.1571 - val_rmse: 0.3964
Epoch 147/400




600/600 [==============================] - 77s 129ms/step - loss: 0.1396 - rmse: 0.3736 - val_loss: 0.1558 - val_rmse: 0.3947
Epoch 148/400




600/600 [==============================] - 74s 123ms/step - loss: 0.1398 - rmse: 0.3740 - val_loss: 0.1584 - val_rmse: 0.3980
Epoch 149/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1409 - rmse: 0.3754 - val_loss: 0.1720 - val_rmse: 0.4147
Epoch 150/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1424 - rmse: 0.3774 - val_loss: 0.1559 - val_rmse: 0.3948
Epoch 151/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1401 - rmse: 0.3743 - val_loss: 0.1735 - val_rmse: 0.4165
Epoch 152/400



600/600 [==============================] - 74s 124ms/step - loss: 0.1413 - rmse: 0.3759 - val_loss: 0.1623 - val_rmse: 0.4028
Epoch 153/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1393 - rmse: 0.3733 - val_loss: 0.1553 - val_rmse: 0.3941
Epoch 154/400



600/600 [==============================] - 73s 122ms/step - loss: 0.1411 - rmse: 0.3756 - val_loss: 0.1572 - val_rmse: 0.3965
Epoch 155/400




600/600 [==============================] - 74s 123ms/step - loss: 0.1389 - rmse: 0.3727 - val_loss: 0.1565 - val_rmse: 0.3956
Epoch 156/400



600/600 [==============================] - 75s 125ms/step - loss: 0.1392 - rmse: 0.3731 - val_loss: 0.1566 - val_rmse: 0.3957
Epoch 157/400




600/600 [==============================] - 75s 125ms/step - loss: 0.1425 - rmse: 0.3775 - val_loss: 0.1620 - val_rmse: 0.4025
Epoch 158/400



600/600 [==============================] - 75s 124ms/step - loss: 0.1403 - rmse: 0.3746 - val_loss: 0.1605 - val_rmse: 0.4006
Epoch 159/400




600/600 [==============================] - 74s 124ms/step - loss: 0.1405 - rmse: 0.3748 - val_loss: 0.1572 - val_rmse: 0.3965
Epoch 160/400




600/600 [==============================] - 77s 129ms/step - loss: 0.1401 - rmse: 0.3743 - val_loss: 0.1543 - val_rmse: 0.3928
Epoch 161/400



600/600 [==============================] - 74s 124ms/step - loss: 0.1404 - rmse: 0.3747 - val_loss: 0.1605 - val_rmse: 0.4006
Epoch 162/400



600/600 [==============================] - 76s 128ms/step - loss: 0.1384 - rmse: 0.3721 - val_loss: 0.1601 - val_rmse: 0.4001
Epoch 163/400



600/600 [==============================] - 76s 127ms/step - loss: 0.1383 - rmse: 0.3719 - val_loss: 0.1651 - val_rmse: 0.4063
Epoch 164/400



600/600 [==============================] - 75s 125ms/step - loss: 0.1430 - rmse: 0.3782 - val_loss: 0.1578 - val_rmse: 0.3972
Epoch 165/400



600/600 [==============================] - 74s 124ms/step - loss: 0.1401 - rmse: 0.3743 - val_loss: 0.1593 - val_rmse: 0.3992
Epoch 166/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1403 - rmse: 0.3746 - val_loss: 0.1564 - val_rmse: 0.3955
Epoch 167/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1391 - rmse: 0.3729 - val_loss: 0.1582 - val_rmse: 0.3977
Epoch 168/400




600/600 [==============================] - 74s 124ms/step - loss: 0.1405 - rmse: 0.3748 - val_loss: 0.1617 - val_rmse: 0.4022
Epoch 169/400




600/600 [==============================] - 74s 123ms/step - loss: 0.1376 - rmse: 0.3710 - val_loss: 0.1546 - val_rmse: 0.3932
Epoch 170/400



600/600 [==============================] - 73s 123ms/step - loss: 0.1399 - rmse: 0.3741 - val_loss: 0.1619 - val_rmse: 0.4024
Epoch 171/400



600/600 [==============================] - 73s 122ms/step - loss: 0.1398 - rmse: 0.3739 - val_loss: 0.1642 - val_rmse: 0.4052
Epoch 172/400




600/600 [==============================] - 74s 124ms/step - loss: 0.1418 - rmse: 0.3765 - val_loss: 0.1600 - val_rmse: 0.4000
Epoch 173/400




600/600 [==============================] - 74s 124ms/step - loss: 0.1387 - rmse: 0.3724 - val_loss: 0.1591 - val_rmse: 0.3989
Epoch 174/400



600/600 [==============================] - 74s 124ms/step - loss: 0.1409 - rmse: 0.3753 - val_loss: 0.1608 - val_rmse: 0.4010
Epoch 175/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1397 - rmse: 0.3738 - val_loss: 0.1557 - val_rmse: 0.3946
Epoch 176/400



600/600 [==============================] - 76s 127ms/step - loss: 0.1389 - rmse: 0.3727 - val_loss: 0.1554 - val_rmse: 0.3942
Epoch 177/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1384 - rmse: 0.3721 - val_loss: 0.1640 - val_rmse: 0.4050
Epoch 178/400




600/600 [==============================] - 76s 127ms/step - loss: 0.1396 - rmse: 0.3736 - val_loss: 0.1607 - val_rmse: 0.4009
Epoch 179/400




600/600 [==============================] - 76s 127ms/step - loss: 0.1416 - rmse: 0.3763 - val_loss: 0.1730 - val_rmse: 0.4159
Epoch 180/400




600/600 [==============================] - 74s 123ms/step - loss: 0.1399 - rmse: 0.3741 - val_loss: 0.1550 - val_rmse: 0.3937
Epoch 181/400




600/600 [==============================] - 74s 123ms/step - loss: 0.1400 - rmse: 0.3742 - val_loss: 0.1629 - val_rmse: 0.4036
Epoch 182/400




600/600 [==============================] - 73s 122ms/step - loss: 0.1382 - rmse: 0.3718 - val_loss: 0.1596 - val_rmse: 0.3995
Epoch 183/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1405 - rmse: 0.3748 - val_loss: 0.1701 - val_rmse: 0.4124
Epoch 184/400



600/600 [==============================] - 73s 122ms/step - loss: 0.1375 - rmse: 0.3708 - val_loss: 0.1650 - val_rmse: 0.4062
Epoch 185/400



600/600 [==============================] - 74s 124ms/step - loss: 0.1384 - rmse: 0.3720 - val_loss: 0.1628 - val_rmse: 0.4035
Epoch 186/400




600/600 [==============================] - 73s 122ms/step - loss: 0.1415 - rmse: 0.3761 - val_loss: 0.1614 - val_rmse: 0.4018
Epoch 187/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1385 - rmse: 0.3722 - val_loss: 0.1601 - val_rmse: 0.4002
Epoch 188/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1399 - rmse: 0.3741 - val_loss: 0.1663 - val_rmse: 0.4078
Epoch 189/400



600/600 [==============================] - 76s 126ms/step - loss: 0.1391 - rmse: 0.3729 - val_loss: 0.1628 - val_rmse: 0.4034
Epoch 190/400




600/600 [==============================] - 74s 124ms/step - loss: 0.1390 - rmse: 0.3729 - val_loss: 0.1583 - val_rmse: 0.3979
Epoch 191/400




600/600 [==============================] - 74s 123ms/step - loss: 0.1379 - rmse: 0.3713 - val_loss: 0.1580 - val_rmse: 0.3975
Epoch 192/400




600/600 [==============================] - 76s 127ms/step - loss: 0.1381 - rmse: 0.3716 - val_loss: 0.1612 - val_rmse: 0.4016
Epoch 193/400




600/600 [==============================] - 74s 123ms/step - loss: 0.1399 - rmse: 0.3741 - val_loss: 0.1670 - val_rmse: 0.4086
Epoch 194/400



600/600 [==============================] - 76s 127ms/step - loss: 0.1401 - rmse: 0.3743 - val_loss: 0.1636 - val_rmse: 0.4045
Epoch 195/400



600/600 [==============================] - 76s 127ms/step - loss: 0.1391 - rmse: 0.3730 - val_loss: 0.1623 - val_rmse: 0.4029
Epoch 196/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1397 - rmse: 0.3737 - val_loss: 0.1611 - val_rmse: 0.4014
Epoch 197/400



600/600 [==============================] - 74s 124ms/step - loss: 0.1392 - rmse: 0.3731 - val_loss: 0.1620 - val_rmse: 0.4025
Epoch 198/400



600/600 [==============================] - 74s 124ms/step - loss: 0.1378 - rmse: 0.3712 - val_loss: 0.1649 - val_rmse: 0.4061
Epoch 199/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1377 - rmse: 0.3711 - val_loss: 0.1658 - val_rmse: 0.4072
Epoch 200/400



600/600 [==============================] - 73s 123ms/step - loss: 0.1393 - rmse: 0.3732 - val_loss: 0.1739 - val_rmse: 0.4171
Epoch 201/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1412 - rmse: 0.3757 - val_loss: 0.1593 - val_rmse: 0.3992
Epoch 202/400




600/600 [==============================] - 74s 123ms/step - loss: 0.1383 - rmse: 0.3718 - val_loss: 0.1673 - val_rmse: 0.4090
Epoch 203/400




600/600 [==============================] - 74s 123ms/step - loss: 0.1393 - rmse: 0.3732 - val_loss: 0.1544 - val_rmse: 0.3929
Epoch 204/400




600/600 [==============================] - 74s 124ms/step - loss: 0.1383 - rmse: 0.3719 - val_loss: 0.1548 - val_rmse: 0.3935
Epoch 205/400




600/600 [==============================] - 77s 128ms/step - loss: 0.1390 - rmse: 0.3728 - val_loss: 0.1583 - val_rmse: 0.3978
Epoch 206/400



600/600 [==============================] - 74s 124ms/step - loss: 0.1372 - rmse: 0.3705 - val_loss: 0.1571 - val_rmse: 0.3964
Epoch 207/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1376 - rmse: 0.3710 - val_loss: 0.1614 - val_rmse: 0.4018
Epoch 208/400




600/600 [==============================] - 74s 123ms/step - loss: 0.1409 - rmse: 0.3754 - val_loss: 0.1540 - val_rmse: 0.3924
Epoch 209/400



600/600 [==============================] - 74s 124ms/step - loss: 0.1387 - rmse: 0.3724 - val_loss: 0.1672 - val_rmse: 0.4089
Epoch 210/400



600/600 [==============================] - 78s 129ms/step - loss: 0.1390 - rmse: 0.3728 - val_loss: 0.1563 - val_rmse: 0.3953
Epoch 211/400




600/600 [==============================] - 76s 127ms/step - loss: 0.1384 - rmse: 0.3720 - val_loss: 0.1535 - val_rmse: 0.3918
Epoch 212/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1389 - rmse: 0.3726 - val_loss: 0.1626 - val_rmse: 0.4032
Epoch 213/400




600/600 [==============================] - 74s 123ms/step - loss: 0.1371 - rmse: 0.3703 - val_loss: 0.1705 - val_rmse: 0.4129
Epoch 214/400




600/600 [==============================] - 75s 124ms/step - loss: 0.1365 - rmse: 0.3694 - val_loss: 0.1641 - val_rmse: 0.4051
Epoch 215/400




600/600 [==============================] - 73s 122ms/step - loss: 0.1414 - rmse: 0.3761 - val_loss: 0.1721 - val_rmse: 0.4149
Epoch 216/400



600/600 [==============================] - 74s 124ms/step - loss: 0.1384 - rmse: 0.3720 - val_loss: 0.1592 - val_rmse: 0.3990
Epoch 217/400




600/600 [==============================] - 73s 122ms/step - loss: 0.1389 - rmse: 0.3727 - val_loss: 0.1574 - val_rmse: 0.3968
Epoch 218/400



600/600 [==============================] - 75s 125ms/step - loss: 0.1376 - rmse: 0.3710 - val_loss: 0.1642 - val_rmse: 0.4052
Epoch 219/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1390 - rmse: 0.3728 - val_loss: 0.1567 - val_rmse: 0.3958
Epoch 220/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1359 - rmse: 0.3687 - val_loss: 0.1542 - val_rmse: 0.3927
Epoch 221/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1387 - rmse: 0.3724 - val_loss: 0.1572 - val_rmse: 0.3964
Epoch 222/400



600/600 [==============================] - 75s 125ms/step - loss: 0.1384 - rmse: 0.3720 - val_loss: 0.1645 - val_rmse: 0.4055
Epoch 223/400



600/600 [==============================] - 74s 124ms/step - loss: 0.1402 - rmse: 0.3745 - val_loss: 0.1583 - val_rmse: 0.3979
Epoch 224/400



600/600 [==============================] - 77s 128ms/step - loss: 0.1375 - rmse: 0.3708 - val_loss: 0.1566 - val_rmse: 0.3957
Epoch 225/400




600/600 [==============================] - 73s 122ms/step - loss: 0.1395 - rmse: 0.3736 - val_loss: 0.1691 - val_rmse: 0.4112
Epoch 226/400



600/600 [==============================] - 76s 127ms/step - loss: 0.1380 - rmse: 0.3715 - val_loss: 0.1529 - val_rmse: 0.3910
Epoch 227/400



600/600 [==============================] - 76s 127ms/step - loss: 0.1372 - rmse: 0.3704 - val_loss: 0.1577 - val_rmse: 0.3971
Epoch 228/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1368 - rmse: 0.3699 - val_loss: 0.1598 - val_rmse: 0.3998
Epoch 229/400




600/600 [==============================] - 73s 122ms/step - loss: 0.1382 - rmse: 0.3717 - val_loss: 0.1568 - val_rmse: 0.3960
Epoch 230/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1401 - rmse: 0.3743 - val_loss: 0.1617 - val_rmse: 0.4022
Epoch 231/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1383 - rmse: 0.3719 - val_loss: 0.1541 - val_rmse: 0.3925
Epoch 232/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1385 - rmse: 0.3722 - val_loss: 0.1623 - val_rmse: 0.4029
Epoch 233/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1371 - rmse: 0.3702 - val_loss: 0.1641 - val_rmse: 0.4051
Epoch 234/400




600/600 [==============================] - 74s 123ms/step - loss: 0.1389 - rmse: 0.3727 - val_loss: 0.1671 - val_rmse: 0.4087
Epoch 235/400



600/600 [==============================] - 74s 124ms/step - loss: 0.1363 - rmse: 0.3691 - val_loss: 0.1605 - val_rmse: 0.4006
Epoch 236/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1371 - rmse: 0.3703 - val_loss: 0.1586 - val_rmse: 0.3982
Epoch 237/400




600/600 [==============================] - 74s 124ms/step - loss: 0.1402 - rmse: 0.3744 - val_loss: 0.1606 - val_rmse: 0.4008
Epoch 238/400




600/600 [==============================] - 73s 122ms/step - loss: 0.1377 - rmse: 0.3710 - val_loss: 0.1663 - val_rmse: 0.4077
Epoch 239/400



600/600 [==============================] - 74s 124ms/step - loss: 0.1386 - rmse: 0.3723 - val_loss: 0.1591 - val_rmse: 0.3989
Epoch 240/400



600/600 [==============================] - 76s 127ms/step - loss: 0.1380 - rmse: 0.3715 - val_loss: 0.1576 - val_rmse: 0.3970
Epoch 241/400



600/600 [==============================] - 73s 122ms/step - loss: 0.1377 - rmse: 0.3710 - val_loss: 0.1562 - val_rmse: 0.3952
Epoch 242/400



600/600 [==============================] - 76s 127ms/step - loss: 0.1364 - rmse: 0.3694 - val_loss: 0.1581 - val_rmse: 0.3976
Epoch 243/400




600/600 [==============================] - 77s 129ms/step - loss: 0.1369 - rmse: 0.3700 - val_loss: 0.1584 - val_rmse: 0.3980
Epoch 244/400



600/600 [==============================] - 73s 122ms/step - loss: 0.1388 - rmse: 0.3725 - val_loss: 0.1584 - val_rmse: 0.3979
Epoch 245/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1387 - rmse: 0.3725 - val_loss: 0.1649 - val_rmse: 0.4060
Epoch 246/400



600/600 [==============================] - 74s 124ms/step - loss: 0.1380 - rmse: 0.3715 - val_loss: 0.1548 - val_rmse: 0.3935
Epoch 247/400



600/600 [==============================] - 75s 124ms/step - loss: 0.1384 - rmse: 0.3720 - val_loss: 0.1594 - val_rmse: 0.3992
Epoch 248/400



600/600 [==============================] - 74s 123ms/step - loss: 0.1380 - rmse: 0.3715 - val_loss: 0.1568 - val_rmse: 0.3960
Epoch 249/400




600/600 [==============================] - 73s 122ms/step - loss: 0.1364 - rmse: 0.3693 - val_loss: 0.1676 - val_rmse: 0.4094
Epoch 250/400



600/600 [==============================] - 73s 122ms/step - loss: 0.1368 - rmse: 0.3698 - val_loss: 0.1616 - val_rmse: 0.4020
Epoch 251/400




600/600 [==============================] - 73s 122ms/step - loss: 0.1382 - rmse: 0.3718 - val_loss: 0.1567 - val_rmse: 0.3959
Epoch 252/400




600/600 [==============================] - 74s 124ms/step - loss: 0.1403 - rmse: 0.3745 - val_loss: 0.1559 - val_rmse: 0.3948
Epoch 253/400




600/600 [==============================] - 74s 124ms/step - loss: 0.1368 - rmse: 0.3698 - val_loss: 0.1705 - val_rmse: 0.4129
Epoch 254/400




600/600 [==============================] - 74s 124ms/step - loss: 0.1380 - rmse: 0.3715 - val_loss: 0.1544 - val_rmse: 0.3929
Epoch 255/400




600/600 [==============================] - 74s 123ms/step - loss: 0.1375 - rmse: 0.3709 - val_loss: 0.1551 - val_rmse: 0.3939
Epoch 256/400



600/600 [==============================] - 77s 129ms/step - loss: 0.1378 - rmse: 0.3712 - val_loss: 0.1623 - val_rmse: 0.4029
Epoch 257/400



600/600 [==============================] - 73s 123ms/step - loss: 0.1360 - rmse: 0.3688 - val_loss: 0.1641 - val_rmse: 0.4051
Epoch 258/400




600/600 [==============================] - 76s 128ms/step - loss: 0.1364 - rmse: 0.3693 - val_loss: 0.1583 - val_rmse: 0.3979
Epoch 259/400




600/600 [==============================] - 77s 128ms/step - loss: 0.1401 - rmse: 0.3742 - val_loss: 0.1579 - val_rmse: 0.3974
Epoch 260/400



600/600 [==============================] - 77s 129ms/step - loss: 0.1377 - rmse: 0.3711 - val_loss: 0.1695 - val_rmse: 0.4117
Epoch 261/400



600/600 [==============================] - 80s 134ms/step - loss: 0.1376 - rmse: 0.3709 - val_loss: 0.1607 - val_rmse: 0.4009
Epoch 262/400



600/600 [==============================] - 82s 137ms/step - loss: 0.1375 - rmse: 0.3708 - val_loss: 0.1573 - val_rmse: 0.3966
Epoch 263/400



600/600 [==============================] - 82s 137ms/step - loss: 0.1378 - rmse: 0.3712 - val_loss: 0.1648 - val_rmse: 0.4060
Epoch 264/400



600/600 [==============================] - 83s 139ms/step - loss: 0.1364 - rmse: 0.3694 - val_loss: 0.1655 - val_rmse: 0.4068
Epoch 265/400



600/600 [==============================] - 81s 135ms/step - loss: 0.1353 - rmse: 0.3678 - val_loss: 0.1597 - val_rmse: 0.3996
Epoch 266/400



600/600 [==============================] - 78s 130ms/step - loss: 0.1403 - rmse: 0.3745 - val_loss: 0.1725 - val_rmse: 0.4153
Epoch 267/400



600/600 [==============================] - 77s 128ms/step - loss: 0.1373 - rmse: 0.3706 - val_loss: 0.1620 - val_rmse: 0.4025
Epoch 268/400



600/600 [==============================] - 77s 129ms/step - loss: 0.1378 - rmse: 0.3712 - val_loss: 0.1517 - val_rmse: 0.3895
Epoch 269/400




600/600 [==============================] - 78s 130ms/step - loss: 0.1368 - rmse: 0.3698 - val_loss: 0.1648 - val_rmse: 0.4059
Epoch 270/400




600/600 [==============================] - 77s 129ms/step - loss: 0.1381 - rmse: 0.3716 - val_loss: 0.1658 - val_rmse: 0.4071
Epoch 271/400



600/600 [==============================] - 78s 130ms/step - loss: 0.1347 - rmse: 0.3671 - val_loss: 0.1604 - val_rmse: 0.4005
Epoch 272/400



600/600 [==============================] - 82s 136ms/step - loss: 0.1375 - rmse: 0.3709 - val_loss: 0.1566 - val_rmse: 0.3958
Epoch 273/400



600/600 [==============================] - 77s 129ms/step - loss: 0.1377 - rmse: 0.3710 - val_loss: 0.1650 - val_rmse: 0.4062
Epoch 274/400




600/600 [==============================] - 82s 136ms/step - loss: 0.1392 - rmse: 0.3731 - val_loss: 0.1620 - val_rmse: 0.4024
Epoch 275/400



600/600 [==============================] - 82s 138ms/step - loss: 0.1363 - rmse: 0.3692 - val_loss: 0.1573 - val_rmse: 0.3966
Epoch 276/400



600/600 [==============================] - 80s 133ms/step - loss: 0.1388 - rmse: 0.3726 - val_loss: 0.1623 - val_rmse: 0.4029
Epoch 277/400



600/600 [==============================] - 79s 132ms/step - loss: 0.1371 - rmse: 0.3703 - val_loss: 0.1527 - val_rmse: 0.3908
Epoch 278/400




600/600 [==============================] - 80s 133ms/step - loss: 0.1364 - rmse: 0.3693 - val_loss: 0.1651 - val_rmse: 0.4063
Epoch 279/400




600/600 [==============================] - 80s 134ms/step - loss: 0.1359 - rmse: 0.3686 - val_loss: 0.1563 - val_rmse: 0.3954
Epoch 280/400



600/600 [==============================] - 82s 138ms/step - loss: 0.1373 - rmse: 0.3705 - val_loss: 0.1572 - val_rmse: 0.3965
Epoch 281/400



600/600 [==============================] - 83s 139ms/step - loss: 0.1390 - rmse: 0.3729 - val_loss: 0.1535 - val_rmse: 0.3918
Epoch 282/400




600/600 [==============================] - 84s 140ms/step - loss: 0.1375 - rmse: 0.3708 - val_loss: 0.1532 - val_rmse: 0.3915
Epoch 283/400




600/600 [==============================] - 86s 143ms/step - loss: 0.1375 - rmse: 0.3708 - val_loss: 0.1557 - val_rmse: 0.3946
Epoch 284/400



600/600 [==============================] - 84s 140ms/step - loss: 0.1366 - rmse: 0.3696 - val_loss: 0.1586 - val_rmse: 0.3983
Epoch 285/400



600/600 [==============================] - 84s 140ms/step - loss: 0.1380 - rmse: 0.3714 - val_loss: 0.1668 - val_rmse: 0.4084
Epoch 286/400



600/600 [==============================] - 85s 142ms/step - loss: 0.1351 - rmse: 0.3676 - val_loss: 0.1643 - val_rmse: 0.4053
Epoch 287/400




600/600 [==============================] - 85s 142ms/step - loss: 0.1364 - rmse: 0.3693 - val_loss: 0.1613 - val_rmse: 0.4016
Epoch 288/400



600/600 [==============================] - 87s 145ms/step - loss: 0.1392 - rmse: 0.3731 - val_loss: 0.1576 - val_rmse: 0.3970
Epoch 289/400




600/600 [==============================] - 83s 139ms/step - loss: 0.1367 - rmse: 0.3697 - val_loss: 0.1614 - val_rmse: 0.4017
Epoch 290/400



600/600 [==============================] - 89s 148ms/step - loss: 0.1378 - rmse: 0.3712 - val_loss: 0.1712 - val_rmse: 0.4138
Epoch 291/400



600/600 [==============================] - 83s 139ms/step - loss: 0.1369 - rmse: 0.3700 - val_loss: 0.1564 - val_rmse: 0.3955
Epoch 292/400




600/600 [==============================] - 85s 142ms/step - loss: 0.1368 - rmse: 0.3699 - val_loss: 0.1586 - val_rmse: 0.3982
Epoch 293/400




600/600 [==============================] - 84s 140ms/step - loss: 0.1358 - rmse: 0.3686 - val_loss: 0.1552 - val_rmse: 0.3940
Epoch 294/400



600/600 [==============================] - 86s 144ms/step - loss: 0.1358 - rmse: 0.3685 - val_loss: 0.1657 - val_rmse: 0.4071
Epoch 295/400



600/600 [==============================] - 81s 136ms/step - loss: 0.1382 - rmse: 0.3718 - val_loss: 0.1591 - val_rmse: 0.3989
Epoch 296/400




600/600 [==============================] - 82s 138ms/step - loss: 0.1379 - rmse: 0.3713 - val_loss: 0.1626 - val_rmse: 0.4032
Epoch 297/400



600/600 [==============================] - 83s 139ms/step - loss: 0.1370 - rmse: 0.3701 - val_loss: 0.1522 - val_rmse: 0.3901
Epoch 298/400




600/600 [==============================] - 85s 143ms/step - loss: 0.1374 - rmse: 0.3706 - val_loss: 0.1568 - val_rmse: 0.3960
Epoch 299/400




600/600 [==============================] - 83s 139ms/step - loss: 0.1372 - rmse: 0.3704 - val_loss: 0.1550 - val_rmse: 0.3937
Epoch 300/400



600/600 [==============================] - 88s 148ms/step - loss: 0.1351 - rmse: 0.3676 - val_loss: 0.1649 - val_rmse: 0.4061
Epoch 301/400



600/600 [==============================] - 84s 140ms/step - loss: 0.1358 - rmse: 0.3686 - val_loss: 0.1588 - val_rmse: 0.3984
Epoch 302/400



600/600 [==============================] - 84s 141ms/step - loss: 0.1373 - rmse: 0.3705 - val_loss: 0.1573 - val_rmse: 0.3966
Epoch 303/400



600/600 [==============================] - 83s 139ms/step - loss: 0.1395 - rmse: 0.3735 - val_loss: 0.1542 - val_rmse: 0.3926
Epoch 304/400




600/600 [==============================] - 83s 139ms/step - loss: 0.1357 - rmse: 0.3684 - val_loss: 0.1647 - val_rmse: 0.4058
Epoch 305/400



600/600 [==============================] - 90s 150ms/step - loss: 0.1375 - rmse: 0.3708 - val_loss: 0.1540 - val_rmse: 0.3925
Epoch 306/400



600/600 [==============================] - 88s 148ms/step - loss: 0.1365 - rmse: 0.3695 - val_loss: 0.1531 - val_rmse: 0.3912
Epoch 307/400



600/600 [==============================] - 83s 139ms/step - loss: 0.1366 - rmse: 0.3697 - val_loss: 0.1617 - val_rmse: 0.4022
Epoch 308/400




600/600 [==============================] - 83s 138ms/step - loss: 0.1353 - rmse: 0.3678 - val_loss: 0.1614 - val_rmse: 0.4018
Epoch 309/400



600/600 [==============================] - 84s 141ms/step - loss: 0.1357 - rmse: 0.3683 - val_loss: 0.1554 - val_rmse: 0.3942
Epoch 310/400



600/600 [==============================] - 83s 138ms/step - loss: 0.1393 - rmse: 0.3732 - val_loss: 0.1626 - val_rmse: 0.4032
Epoch 311/400




600/600 [==============================] - 82s 137ms/step - loss: 0.1365 - rmse: 0.3695 - val_loss: 0.1588 - val_rmse: 0.3985
Epoch 312/400




600/600 [==============================] - 84s 139ms/step - loss: 0.1370 - rmse: 0.3702 - val_loss: 0.1603 - val_rmse: 0.4003
Epoch 313/400



600/600 [==============================] - 83s 139ms/step - loss: 0.1363 - rmse: 0.3692 - val_loss: 0.1558 - val_rmse: 0.3947
Epoch 314/400




600/600 [==============================] - 81s 135ms/step - loss: 0.1369 - rmse: 0.3699 - val_loss: 0.1611 - val_rmse: 0.4013
Epoch 315/400



600/600 [==============================] - 80s 134ms/step - loss: 0.1353 - rmse: 0.3678 - val_loss: 0.1539 - val_rmse: 0.3923
Epoch 316/400



600/600 [==============================] - 88s 146ms/step - loss: 0.1347 - rmse: 0.3671 - val_loss: 0.1642 - val_rmse: 0.4052
Epoch 317/400




600/600 [==============================] - 80s 134ms/step - loss: 0.1393 - rmse: 0.3733 - val_loss: 0.1656 - val_rmse: 0.4070
Epoch 318/400




600/600 [==============================] - 81s 135ms/step - loss: 0.1364 - rmse: 0.3694 - val_loss: 0.1576 - val_rmse: 0.3970


































2210/2210 [==============================] - 70s 32ms/step - loss: 0.1517 - rmse: 0.3895
New model saved.
Min: -0.3333333 Max: 0.30261758
Min: -0.2916928 Max: 0.33333334
Min: 3.0009497e-09 Max: 11.036943
(129, 436)
predictors.shape: (436, 129, 8, 1)
(436, 129, 1, 1)
Min: -0.23926671 Max: 0.2656372